interview <- read.csv("C:/Users/arund/OneDrive/Desktop/Masters in Business Analytics/Text Analytics/Class 4/tiktok.csv")
interview$Question.1 <- as.character(interview$Question.1)
interview$Question.2 <- as.character(interview$Question.2)
interview$Question.3 <- as.character(interview$Question.3)
interview$Question.4 <- as.character(interview$Question.4)
interview$Question.5 <- as.character(interview$Question.5)
str(interview)
View(interview)


library(dplyr)
library(stringr)
library(tidytext)

data(stop_words)

# Question 1: What kind of videos do you watch and why?

frequencies_tokens_nostop1 <- interview %>%
  unnest_tokens(word, Question.1) %>%
  anti_join(stop_words) %>% #here's where we remove tokens
  count(word, sort=TRUE)

print(frequencies_tokens_nostop1) # This is Tidy Format

# Question 2: What kind of social media apps are you using and why?

frequencies_tokens_nostop2 <- interview %>%
  unnest_tokens(word, Question.2) %>%
  anti_join(stop_words) %>% #here's where we remove tokens
  count(word, sort=TRUE)

print(frequencies_tokens_nostop2)

# Question 3: About how many hours do you spend on social media and which days?

frequencies_tokens_nostop3 <- interview %>%
  unnest_tokens(word, Question.3) %>%
  anti_join(stop_words) %>% #here's where we remove tokens
  count(word, sort=TRUE)

print(frequencies_tokens_nostop3)

# Question 4: What hashtags have you used the most?

frequencies_tokens_nostop4 <- interview %>%
  unnest_tokens(word, Question.4) %>%
  anti_join(stop_words) %>% #here's where we remove tokens
  count(word, sort=TRUE)

print(frequencies_tokens_nostop4)

# Question 5: Will you use TikTok?

frequencies_tokens_nostop5 <- interview %>%
  unnest_tokens(word, Question.5) %>%
  anti_join(stop_words) %>% #here's where we remove tokens
  count(word, sort=TRUE)

print(frequencies_tokens_nostop5)

# Bind all to single data

my_df <- bind_rows(
  mutate(frequencies_tokens_nostop1, question = "first"),
  mutate(frequencies_tokens_nostop2, question = "second"),
  mutate(frequencies_tokens_nostop3, question = "third"),
  mutate(frequencies_tokens_nostop4, question = "fourth"),
  mutate(frequencies_tokens_nostop5, question = "fifth")
)

# Cast DTM

mydf_dtm <- my_df %>%
  cast_dtm(question, word, n)

mydf_dtm

#calling the Latent Dirichlet Allocation algorithm
mydf_lda <- LDA(mydf_dtm, k=2, control=list(seed=123))
mydf_lda

#now we are looking for the per topic per word probabilities aka. beta
#beta - what is the probability that "this term" will be generated by "this topic"
library(tidytext)
mydf_topics <- tidy(mydf_lda, matrix="beta")
mydf_topics

library(ggplot2)
library(dplyr)

top_terms <- mydf_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
top_terms

#lets plot the term frequencies by topic
top_terms %>%
  mutate(term=reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend=FALSE) +
  facet_wrap(~topic, scales = "free") +
  coord_flip()

#lets calculate the relative difference between the betas for words in topic 1
#and words in topic 2
library(tidyr)
beta_spread <- mydf_topics %>%
  mutate(topic=paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1>.001 | topic2 >0.001) %>%
  mutate(log_rate = log2(topic2/topic1)) %>%
  #arrange(log_rate) # belong to topic 1
  arrange(desc(log_rate)) # belong to topic 2

beta_spread

my_gamma <- tidy(mydf_lda, matrix = "gamma") # Document close to which topic by seeing probability
# See p, 1-p

my_gamma %>%
  filter(document == 1)


################################################
###### LDA per chapter based on Gutenber#########
################################################

library(gutenbergr)
titles <- c("Twenty Thousand Leagues under the Sea", "The War of the Worlds",
            "Pride and Prejudice", "Great Expectations")

books <- gutenberg_works(title %in% titles) %>%
  gutenberg_download(meta_fields = "title")

#devide into documents, each representing on e chapter
library(stringr )
reg <-regex("^chapter", ignore_case=TRUE)
by_chapter <- books %>%
  group_by(title) %>%
  mutate(chapter=cumsum(str_detect(text,reg))) %>%
  ungroup() %>%
  filter(chapter>0) %>%
  unite(document, title, chapter)

#split into words
by_chapter_word <- by_chapter %>%
  unnest_tokens(word, text)

#find document-word counts
word_counts <- by_chapter_word %>%
  anti_join(stop_words) %>%
  count(document, word, sort = TRUE) %>%
  ungroup()

print(word_counts)

chapters_dtm <- word_counts %>%
  cast_dtm(document, word, n)

chapters_lda <- LDA(chapters_dtm,k=4, control = list(seed=123))
chapters_lda

##################################################################################################

# Finding per person

transpose_df <- t(interview)
transpose_df <- as.data.frame(transpose_df)
View(transpose_df)
# Person Function

my_func <- function(x){
  frequencies_tokens_nostop <- transpose_df %>%
    unnest_tokens(word, x) %>%
    anti_join(stop_words) %>% #here's where we remove tokens
    count(word, sort=TRUE)
}

a <- my_func(V1)
b <- my_func(V2)
c <- my_func(V3)
d <- my_func(V4)
e <- my_func(V5)
f <- my_func(V6)
g <- my_func(V7)
h <- my_func(V8)
i <- my_func(V9)
j <- my_func(V10)
k <- my_func(V11)
l <- my_func(V12)
m <- my_func(V13)
n <- my_func(V14)
o <- my_func(V15)
p <- my_func(V16)
q <- my_func(V17)
r <- my_func(V18)
s <- my_func(V19)
t <- my_func(V20)


# Bind all to single data

my_df <- bind_rows(
  mutate(frequencies_tokens_nostop1, question = "first"),
  mutate(frequencies_tokens_nostop2, question = "second"),
  mutate(frequencies_tokens_nostop3, question = "third"),
  mutate(frequencies_tokens_nostop4, question = "fourth"),
  mutate(frequencies_tokens_nostop5, question = "fifth")
)

# Cast DTM

mydf_dtm <- my_df %>%
  cast_dtm(question, word, n)

mydf_dtm

#calling the Latent Dirichlet Allocation algorithm
mydf_lda <- LDA(mydf_dtm, k=2, control=list(seed=123))
mydf_lda

#now we are looking for the per topic per word probabilities aka. beta
#beta - what is the probability that "this term" will be generated by "this topic"
library(tidytext)
mydf_topics <- tidy(mydf_lda, matrix="beta")
mydf_topics

library(ggplot2)
library(dplyr)

top_terms <- mydf_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
top_terms

#lets plot the term frequencies by topic
top_terms %>%
  mutate(term=reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend=FALSE) +
  facet_wrap(~topic, scales = "free") +
  coord_flip()

#lets calculate the relative difference between the betas for words in topic 1
#and words in topic 2
library(tidyr)
beta_spread <- mydf_topics %>%
  mutate(topic=paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1>.001 | topic2 >0.001) %>%
  mutate(log_rate = log2(topic2/topic1)) %>%
  #arrange(log_rate) # belong to topic 1
  arrange(desc(log_rate)) # belong to topic 2

beta_spread

my_gamma <- tidy(mydf_lda, matrix = "gamma") # Document close to which topic by seeing probability
# See p, 1-p

my_gamma %>%
  filter(document == 1)


################################################
###### LDA per chapter based on Gutenber#########
################################################

library(gutenbergr)
titles <- c("Twenty Thousand Leagues under the Sea", "The War of the Worlds",
            "Pride and Prejudice", "Great Expectations")

books <- gutenberg_works(title %in% titles) %>%
  gutenberg_download(meta_fields = "title")

#devide into documents, each representing on e chapter
library(stringr )
reg <-regex("^chapter", ignore_case=TRUE)
by_chapter <- books %>%
  group_by(title) %>%
  mutate(chapter=cumsum(str_detect(text,reg))) %>%
  ungroup() %>%
  filter(chapter>0) %>%
  unite(document, title, chapter)

#split into words
by_chapter_word <- by_chapter %>%
  unnest_tokens(word, text)

#find document-word counts
word_counts <- by_chapter_word %>%
  anti_join(stop_words) %>%
  count(document, word, sort = TRUE) %>%
  ungroup()

print(word_counts)

chapters_dtm <- word_counts %>%
  cast_dtm(document, word, n)

chapters_lda <- LDA(chapters_dtm,k=4, control = list(seed=123))
chapters_lda


