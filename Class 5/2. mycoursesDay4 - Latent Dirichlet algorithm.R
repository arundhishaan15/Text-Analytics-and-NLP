################################################
###### Latent Dirichlet algorithm ##############
################################################

# There are two principles:
#1. Every document is a combination of multiple topics
#2. Every topic is a combination of multiple words

#install.packages("topicmodels")
library(topicmodels) #new articles from American news agencies
data("AssociatedPress")
AssociatedPress

#calling the Latent Dirichlet Allocation algorithm
ap_lda <- LDA(AssociatedPress, k=4, control=list(seed=123))
ap_lda

#now we are looking for the per topic per word probabilities aka. beta
#beta - what is the probability that "this term" will be generated by "this topic"
library(tidytext)
ap_topics <- tidy(ap_lda, matrix="beta")
ap_topics
library(ggplot2)
library(dplyr)

top_terms <- ap_topics %>%
              group_by(topic) %>%
              top_n(10, beta) %>%
              ungroup() %>%
              arrange(topic, -beta)
top_terms

#lets plot the term frequencies by topic
top_terms %>%
  mutate(term=reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend=FALSE) +
  facet_wrap(~topic, scales = "free") +
  coord_flip()

#lets calculate the relative difference between the betas for words in topic 1
#and words in topic 2
library(tidyr)
beta_spread <- ap_topics %>%
  mutate(topic=paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1>.001 | topic2 >0.001) %>%
  mutate(log_rate = log2(topic2/topic1)) %>%
  arrange(log_rate) # belong to topic 1
  #arrange(desc(log_rate)) # belong to topic 2

beta_spread

my_gamma <- tidy(ap_lda, matrix = "gamma") # Document close to which topic by seeing probability
# See p, 1-p

my_gamma %>%
  filter(document == 1)
  

################################################
###### LDA per chapter based on Gutenber#########
################################################

library(gutenbergr)
titles <- c("Twenty Thousand Leagues under the Sea", "The War of the Worlds",
            "Pride and Prejudice", "Great Expectations")

books <- gutenberg_works(title %in% titles) %>%
  gutenberg_download(meta_fields = "title")

#devide into documents, each representing on e chapter
library(stringr )
reg <-regex("^chapter", ignore_case=TRUE)
by_chapter <- books %>%
  group_by(title) %>%
  mutate(chapter=cumsum(str_detect(text,reg))) %>%
  ungroup() %>%
  filter(chapter>0) %>%
  unite(document, title, chapter)

#split into words
by_chapter_word <- by_chapter %>%
  unnest_tokens(word, text)

#find document-word counts
word_counts <- by_chapter_word %>%
  anti_join(stop_words) %>%
  count(document, word, sort = TRUE) %>%
  ungroup()

print(word_counts)

chapters_dtm <- word_counts %>%
  cast_dtm(document, word, n)

chapters_lda <- LDA(chapters_dtm,k=4, control = list(seed=123))
chapters_lda

###############################################
#### Per documnet classification ##############
###############################################

chapters_gamma <- tidy(chapters_lda, matrix="gamma")
chapters_gamma 

chapters_gamma <- chapters_gamma %>%
  separate(document, c("title", "chapter"),sep = "_", convert=TRUE)

chapters_gamma

chapters_gamma %>%
  mutate(title=reorder(title, gamma*topic)) %>%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot()+
  facet_wrap(~title)

chapter_classifications <- chapters_gamma %>%
  group_by(title, chapter) %>%
  top_n(1, gamma) %>%
  ungroup()

chapter_classifications

